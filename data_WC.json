{"paragraphs":[{"text":"%pyspark\n\nfrom pyspark.sql import Row\n\n# Load a text file and convert each line to a Row.\nlines = sc.textFile(\"wasb:///data/assignemt/sample_data.txt\")\n\nparts = lines.map(lambda l: l.split(\" \"))\nlogData = parts.map(lambda p: Row(clientId=p[0], logDate=p[3], url=p[6]))\n ","dateUpdated":"2019-01-06T10:20:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1546675132279_0010<br/>Spark WebUI: <a href=\"http://hn1-zeppli.j3wsbx35qphu1mdwyn24n3o5gb.cx.internal.cloudapp.net:8088/proxy/application_1546675132279_0010/\">http://hn1-zeppli.j3wsbx35qphu1mdwyn24n3o5gb.cx.internal.cloudapp.net:8088/proxy/application_1546675132279_0010/</a>"}]},"apps":[],"jobName":"paragraph_1546770055521_-2118259935","id":"20190104-120154_64315698","dateCreated":"2019-01-06T10:20:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1135"},{"text":"%pyspark\n","dateUpdated":"2019-01-06T10:20:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1546770055521_-2118259935","id":"20190104-145843_1889165500","dateCreated":"2019-01-06T10:20:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1136"},{"text":"%pyspark\n","dateUpdated":"2019-01-06T10:20:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1546770055522_-2117105689","id":"20190104-154249_1531431065","dateCreated":"2019-01-06T10:20:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1137"},{"text":"%pyspark\n\n## Infer the schema, and register the DataFrame as a table.\ndtSchemaLogs = spark.createDataFrame(logData)\ndtSchemaLogs.createOrReplaceTempView(\"logData\")\n\n# SQL can be run over DataFrames that have been registered as a table.\nlogCleanData = spark.sql(\"SELECT clientId, TO_DATE(CAST(UNIX_TIMESTAMP(substr(logDate,2,21), 'dd/MMM/yyyy:HH:mm:ss') AS TIMESTAMP)) AS logDate, url FROM logData where url rlike '/(english|french).*(.htm|html)'\")\n\n#logRawData = spark.sql(\"SELECT clientId, unix_timestamp(substr(logDate,2,21),'dd/MMM/yyyy:HH:mm:ss'), url FROM logData where url rlike '/(english|french).*(.htm|html)'\")\n\n# The results of SQL queries are Dataframe objects.\n# rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.\nallIds = logCleanData.rdd.map(lambda p: \"clientId: \" + p.clientId).collect()\n#allDates = logRawData.rdd.map(lambda p: \"dateStringPart: \" + p.dateStringPart).collect()\nallUrls = logCleanData.rdd.map(lambda p: \"url: \" + p.url).collect()\n\n\n## Infer the schema, and register the DataFrame as a table.\n#dtSchemaCleanLogs = spark.createDataFrame(logCleanData)\n#dtSchemaCleanLogs.createOrReplaceTempView(\"logCleanData\")\n\nlogCleanData.select(\"url\").show()\nlogCleanData.createOrReplaceTempView(\"logCleanData\")","dateUpdated":"2019-01-06T10:20:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+\n|                 url|\n+--------------------+\n|/english/splash_i...|\n|/french/nav_top_i...|\n|/english/nav_top_...|\n|/english/splash_i...|\n|/english/news/new...|\n| /english/index.html|\n|/english/teams/te...|\n| /english/index.html|\n|/english/teams/te...|\n|/english/nav_top_...|\n|/english/news/jer...|\n|/english/teams/te...|\n|/english/nav_top_...|\n|/english/news/050...|\n| /english/index.html|\n|/english/news/050...|\n|/english/news/290...|\n|/english/teams/te...|\n| /english/index.html|\n|  /french/index.html|\n+--------------------+\nonly showing top 20 rows"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1546675132279_0010<br/>Spark WebUI: <a href=\"http://hn1-zeppli.j3wsbx35qphu1mdwyn24n3o5gb.cx.internal.cloudapp.net:8088/proxy/application_1546675132279_0010/\">http://hn1-zeppli.j3wsbx35qphu1mdwyn24n3o5gb.cx.internal.cloudapp.net:8088/proxy/application_1546675132279_0010/</a>"}]},"apps":[],"jobName":"paragraph_1546770055522_-2117105689","id":"20190104-120343_790527840","dateCreated":"2019-01-06T10:20:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1138"},{"text":"%pyspark\n\n#allDates = logRawData.rdd.map(lambda p: p.logDate).collect() \n\n\nlogCleanData.show()\n    \n    \n","dateUpdated":"2019-01-06T10:20:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+----------+--------------------+\n|clientId|   logDate|                 url|\n+--------+----------+--------------------+\n|  712640|1998-06-06|/english/splash_i...|\n|  347309|1998-06-06|/french/nav_top_i...|\n|  331609|1998-06-06|/english/nav_top_...|\n|  567510|1998-06-06|/english/splash_i...|\n|   71916|1998-06-06|/english/news/new...|\n|  902544|1998-06-06| /english/index.html|\n|   15425|1998-06-06|/english/teams/te...|\n|    6903|1998-06-06| /english/index.html|\n|  706341|1998-06-06|/english/teams/te...|\n|  902548|1998-06-06|/english/nav_top_...|\n|    1734|1998-06-06|/english/news/jer...|\n|   65091|1998-06-06|/english/teams/te...|\n|      65|1998-06-06|/english/nav_top_...|\n|  253789|1998-06-06|/english/news/050...|\n|    1195|1998-06-06| /english/index.html|\n|   25339|1998-06-06|/english/news/050...|\n|   15425|1998-06-06|/english/news/290...|\n|  603033|1998-06-06|/english/teams/te...|\n|  236446|1998-06-06| /english/index.html|\n| 1026316|1998-06-06|  /french/index.html|\n+--------+----------+--------------------+\nonly showing top 20 rows"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1546675132279_0010<br/>Spark WebUI: <a href=\"http://hn1-zeppli.j3wsbx35qphu1mdwyn24n3o5gb.cx.internal.cloudapp.net:8088/proxy/application_1546675132279_0010/\">http://hn1-zeppli.j3wsbx35qphu1mdwyn24n3o5gb.cx.internal.cloudapp.net:8088/proxy/application_1546675132279_0010/</a>"}]},"apps":[],"jobName":"paragraph_1546770055522_-2117105689","id":"20190104-145513_1119305166","dateCreated":"2019-01-06T10:20:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1139"},{"text":"%pyspark\n\n\nfrom pyspark.sql import Row\n\n\ndfTeamSchedule = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"wasb:///data/assignemt/schedulevTeams.csv\")\ndfTeamSchedule.createOrReplaceTempView(\"dfTeamSchedule\")\n\n#dfTeamSchedule.show()\n\n\n## Infer the schema, and register the DataFrame as a table.\n#dfTeamSchedule = spark.createDataFrame(teamSchedule)","dateUpdated":"2019-01-06T10:20:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1546675132279_0010<br/>Spark WebUI: <a href=\"http://hn1-zeppli.j3wsbx35qphu1mdwyn24n3o5gb.cx.internal.cloudapp.net:8088/proxy/application_1546675132279_0010/\">http://hn1-zeppli.j3wsbx35qphu1mdwyn24n3o5gb.cx.internal.cloudapp.net:8088/proxy/application_1546675132279_0010/</a>"}]},"apps":[],"jobName":"paragraph_1546770055522_-2117105689","id":"20190104-152925_999999339","dateCreated":"2019-01-06T10:20:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1140"},{"text":"%pyspark\nfrom pyspark.ml.feature import CountVectorizer\nfrom numpy import array\n\nlogDataWithTeams = spark.sql(\"SELECT int(L.clientId) as clientId, int(S.Team1) as team1, int(S.Team2) as team2 from logCleanData L inner join dfTeamSchedule S on L.logDate = S.logDate\")\nlogDataWithTeams.show()\n\n#logDataWithTeams = logDataWithTeams.rdd.map(lambda p:array([float(p.clientId), float(p.team1), float(p.team2)]))\n#logDataWithTeams = logDataWithTeams.rdd.map(lambda p: (p.clientId, p.team1, p.team2))\n\nlogDataWithTeams = logDataWithTeams.rdd.map(lambda p: (int(p.clientId), int(p.team1), int(p.team2)))\n \n# collect the RDD to a list\nllist = logDataWithTeams.collect()\n \n# print the list\n#for line in llist:\n#    print line\n\ndfLogData = spark.createDataFrame(logDataWithTeams)\n\n\n\n\n\n","dateUpdated":"2019-01-06T10:20:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+-----+-----+\n|clientId|team1|team2|\n+--------+-----+-----+\n|  302955|   15|   16|\n|  302955|    1|   31|\n| 1405038|   15|   16|\n| 1405038|    1|   31|\n|  368798|   15|   16|\n|  368798|    1|   31|\n| 1912227|   15|   16|\n| 1912227|    1|   31|\n|     367|   15|   16|\n|     367|    1|   31|\n|    8463|   15|   16|\n|    8463|    1|   31|\n|  738748|   15|   16|\n|  738748|    1|   31|\n|   27402|   15|   16|\n|   27402|    1|   31|\n|    2715|   15|   16|\n|    2715|    1|   31|\n| 1141754|   15|   16|\n| 1141754|    1|   31|\n+--------+-----+-----+\nonly showing top 20 rows"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1546675132279_0010<br/>Spark WebUI: <a href=\"http://hn1-zeppli.j3wsbx35qphu1mdwyn24n3o5gb.cx.internal.cloudapp.net:8088/proxy/application_1546675132279_0010/\">http://hn1-zeppli.j3wsbx35qphu1mdwyn24n3o5gb.cx.internal.cloudapp.net:8088/proxy/application_1546675132279_0010/</a>"}]},"apps":[],"jobName":"paragraph_1546770055522_-2117105689","id":"20190105-054902_1361482554","dateCreated":"2019-01-06T10:20:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1141"},{"text":"%pyspark\nfrom pyspark.ml.clustering import KMeans\n\n\n\n\n# Trains a k-means model.\nkmeans = KMeans().setK(2).setSeed(1)\nmodel = kmeans.fit(dfLogData)\n\n# Evaluate clustering by computing Within Set Sum of Squared Errors.\n#wssse = model.computeCost(logDataWithTeams)\n#print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n\n\n# Shows the result.\n#centers = model.clusterCenters()\n#print(\"Cluster Centers: \")\n#for center in centers:\n#    print(center)\n","dateUpdated":"2019-01-06T10:20:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"u'Field \"features\" does not exist.\\nAvailable fields: _1, _2, _3'\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\", line 132, in fit\n    return self._fit(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 288, in _fit\n    java_model = self._fit_java(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 285, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\nIllegalArgumentException: u'Field \"features\" does not exist.\\nAvailable fields: _1, _2, _3'\n"}]},"apps":[],"jobName":"paragraph_1546770055522_-2117105689","id":"20190105-060630_1601415600","dateCreated":"2019-01-06T10:20:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1142"},{"text":"%pyspark\n\n# Trains a k-means model.\nkmeans = KMeans().setK(2).setSeed(1)\nmodel = kmeans.fit(dfLogData)\n","dateUpdated":"2019-01-06T10:20:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"u'Field \"features\" does not exist.\\nAvailable fields: _1, _2, _3'\nTraceback (most recent call last):\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/base.py\", line 132, in fit\n    return self._fit(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 288, in _fit\n    java_model = self._fit_java(dataset)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/ml/wrapper.py\", line 285, in _fit_java\n    return self._java_obj.fit(dataset._jdf)\n  File \"/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py\", line 79, in deco\n    raise IllegalArgumentException(s.split(': ', 1)[1], stackTrace)\nIllegalArgumentException: u'Field \"features\" does not exist.\\nAvailable fields: _1, _2, _3'\n"}]},"apps":[],"jobName":"paragraph_1546770055522_-2117105689","id":"20190105-082815_1822826524","dateCreated":"2019-01-06T10:20:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1143"},{"text":"%pyspark\n","dateUpdated":"2019-01-06T10:20:55+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1546770055522_-2117105689","id":"20190106-092416_2024335548","dateCreated":"2019-01-06T10:20:55+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1144"}],"name":"harini_WC","id":"2E31EWR7B","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}